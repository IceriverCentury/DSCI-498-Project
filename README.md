# DSCI-498-Project

# Project Title: DSCI-498-Project

# Project Abstract
## Effectively detecting aggressive and abusive language online has become an essential task in today’s social media platforms, where fostering a safe and healthy community is a priority. The rise of toxic behaviors and harmful content on these platforms calls for advanced mechanisms to identify and address such issues. Over the years, the natural language processing (NLP) community has made significant strides in developing various abuse detection methods with promising results. However, most of these methods focus primarily on isolated components, such as the linguistic properties of comments or the characteristics of the online communities involved. These approaches tend to overlook a crucial aspect: the complex relationship between the language people use and their emotional states. This research aims to bridge that gap by proposing a joint model for emotion and abusive language detection. The goal is to explore how these two factors—emotions and abusive language—can be detected together, enhancing the overall understanding and identification of harmful content. The project will employ a multi-task learning (MTL) framework, where tasks related to emotion detection and abusive language identification will inform and influence each other. By incorporating affective features, the model aims to improve performance, surpassing the capabilities of traditional single-task models. The Single-Task Learning (STL) model will serve as the baseline for comparison. So far, preliminary results with the STL model have been promising, showing that the basic approach can identify abusive language effectively. Moving forward, the research will focus on implementing the MTL model and testing whether it can outperform the STL baseline. This step is crucial to determining whether joint emotion and abusive language detection can lead to better overall outcomes in understanding and addressing harmful content online. The success of this approach could significantly contribute to the development of more sophisticated and accurate tools for social media platforms to protect their users and foster safer online environments.

# Single Task Learning
## As our baselines, we use different Single-Task Learning (STL) models that utilize abuse detection as the sole optimization objective. Those STL models include Simple Linear classifier and Bidirectional Long Short Term Memory (BiLSTM) and Attention classifier. The inputs to both networks will be a sequence of words which are converted into k-dimensional word embeddings using GloVe embedding. They also both use a sigmoid to determine a binary prediction of the hate speech output. The network parameters are optimized to minimize binary cross entropy loss as defined by pytorch. The function is shown in the picture below. We use a LSTM for our models because we need a network with a time component and can remember all of the words in context with another. Bidirectionality helps due to the long length of the possible sentences, and because sometimes the context for determining the meaning of the sentences can be more easily interpreted while reading it backwards.  Our simple linear classifier takes the output of the first BiLSTM and directly produces results from its meaning. The BiLSTM and attention classifier takes the output of the stacked biLSTM and feeds it into another BiLSTM. The output of this layer feeds into an attention layer, which weights each word in the sequence, and attempts to aggregate these hidden states to create a context vector. This context vector will theoretically have a better accuracy than using the hidden state by itself.

# Pretrained Word Vector:  https://nlp.stanford.edu/projects/glove/